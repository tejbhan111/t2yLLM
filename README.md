# <u>**t2yLLM : a fast LLM based Voice Assistant**</u>


## <u>üí° What it does :</u>

- **t2yLLM** lets you speak to your device of choice (here a raspberry Pi with a respeaker hat from seeed studio)
and get an audio answer from your favorite LLM (here Qwen3 by default).
It should just work like any home assistant.
The default keyword to activate speech detection is **"Ok Mars"**
but you can change it of course.
ATM if you want a custom keyword, it is **mandatory** to create a Picovoice account (check [Picovoice](https://picovoice.ai/)), train and download a custom keyword to get a working pipeline.
- **Meteo** : It can search for meteo infos using your OpenWeather API key
- **Pokemon** : Look for any Pokemon info using Tyradex API (french)
- **Wikipedia** : Make Wikipedia searches using the python API
- **Vector Search** : stores all in a synthetic way in chromadb if needed and can retrieve the memorized info
- **t2yLLM** is meant to work on a 16GB GPU, but in order to achieve that, first launch the LLM backend script in order to avoid OOM

## <u>üí° How it works : </u>

- **llm_backend_async.py** receives user prompts (text/str), generates an answer
  (token by token with the async engine of vllm) and forwards it to the dispatcher.
  this script should be installed on your server/desktop.
  
- the **dispatcher.py** script runs Faster-Whisper, piperTTS, silero and porcupine. It is responsible for :
  -transforming answers generated by llm_backend_async.py to audio chunks (in .flac via piperTTS) and send them over udp to your
  raspberry Pi.
  -getting audio from the raspberry Pi and converting it to text via Faster-Whisper with as low latency as possible.
  this script should be installed on your server/desktop (by default, the same as llm_backend_async.py but it can be different).
  
-the **rpi_server.py** program should be copied and run on the raspberry Pi. It is responsible for :
  - getting audio from the respeaker lite and forwarding it to dispatcher.py
  - getting audio (.flac chunks) generated by dispatcher.py and play it the speaker linked to your respeaker lite (or device of choice)
-the .yaml config files should be used to tweak paramaters like silence, models, directories etc...

- You can check models I use in the config files and also in the faster_whisper directory. **To make things work on a 16GB gpu 
it needs quantization. You also need to fully load the llm_back_async.py first**. Should have no problem on 24GB GPUs.

- Different parameters of vLLM can be used to save VRAM like enforce_eager, max_model_len etc... vLLM documentation is very rich

![t2yLLM](https://github.com/user-attachments/assets/21c1988d-dd92-48d8-8632-fe34aa4b4188)



## <u>üî• Backends :</u>

- **vLLM** : really fast and well documented inference pipeline for your favorite LLM
- **Faster-Whisper** : incredibly fast STT for realtime text generation
- **piperTTS** : fast text to speech generating natural voice, maybe the best for french atm
- **Silero-vad** : process the audio buffer and prevents whisper hallucinations
- **pvporcupine** : keyword detection
- **Chromadb** : a vector search database that serves as the model memory
- **default LLM** : Qwen3 14B or other variants, GPTQ 4Bit quantized
- **Pytorch**

## <u>üí° Specifics :</u>

###  Material :
  - **client side** :
    - Raspberry Pi 5, 4GB
    - respeaker like from seeed studio
  - **server side** :
    - An Nvidia GPU with 16GB of VRAM (mini)

###  Pipeline :
  - **t2yLLM** uses AsyncLLMEngine in order to stream tokens and generate sound from them as soon as possible.
  - The audio dispatcher processes text received from the LLM and transforms it to .flac segments and then
    sends them to the client (raspberry Pi)
  - Sound reveived from the Pi 5 is analyzed by silerovad to detect speech in addition to pvporcupine
  - Relevant sound is then translated by Faster-Whisper with low latency
  - The audio dispatcher transforms the LLM answer to speech with coqui TTS and then sends audio parts in .flac
    over the network to reduce bandwidth usage and decrease latency

## <u>‚öôÔ∏è Parameters :</u>

- configuration should be done via the .yaml config file without having to directly interact with the code
- configuration can be enhanced via the YamlConfig_Loader.py
- **t2yLLM** should be used on local network only since all is in clear text for now
- the directory structure should be /t2yLLM/config and /t2yLLM/Chat

## <u>‚öôÔ∏è Environment variables :</u>

create a .env file and use python-dotenv or edit your ~/.bashrc :

- export PORCUPINE_KEY='myporcupinekey'
- export OPENWEATHERMAP_API_KEY='myopenweatherkey'
- export VLLM_ATTENTION_BACKEND=FLASH_ATTN #for V1 engine
- export VLLM_FLASH_ATTN_VERSION="2"       #for V1 engine
- export VLLM_USE_V1=1
- VLLM_WORKER_MULTIPROC_METHOD="spawn" #for V1 engine
- export TORCH_CUDA_ARCH_LIST='myarchitecture' #if needed


## <u>üîç Github repositories used in order to make this code : </u>

- üîó [vLLM](https://github.com/vllm-project/vllm)
- üîó [Faster-Whisper](https://github.com/SYSTRAN/faster-whisper)
- üîó [Silero-vad](https://github.com/snakers4/silero-vad)
- üîó [pvporcupine](https://github.com/Picovoice/porcupine)
- üîó [Whisper-streaming](https://github.com/ufal/whisper_streaming)
- üîó [RealtimeSTT](https://github.com/KoljaB/RealtimeSTT)
- üîó [Chromadb](https://github.com/chroma-core/chroma)
- üîó [piperTTS](https://github.com/rhasspy/piper)
- üîó [json_repair](https://github.com/mangiucugna/json_repair)

## <u>üîç APIs :</u>

- üîó [Tyradex](https://tyradex.vercel.app/)
- üîó [OpenWeather](https://openweathermap.org/)

## <u>üõ†Ô∏è ToDo : </u>

- Keep context between interactions
- improve processing pipeline
- English pokemon API (and .txt as well as phonetics)
- Switch from UDP to Quic
- switch or add option to use OpenWakeWord but I could not make it train on a custom wake word

## <u>‚öñÔ∏è License :</u>

This code is under the **MIT** license. Please mention me as the author if you found this code useful
  [![MIT License](https://img.shields.io/badge/license-MIT-green)](https://opensource.org/licenses/MIT)

Copyright (c) 2025 Saga9103

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
